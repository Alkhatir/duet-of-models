output_dir: "./runs/tiny-gpt2-micro"
seed: 42

model:
  architecture: "gpt2"
  vocab_size: 1024
  hidden_size: 384
  num_hidden_layers: 6
  num_attention_heads: 6          # heads *must* divide hidden_size
  max_position_embeddings: 1024

train:
  per_device_train_batch_size: 6   # slightly heavier, lower batch
  per_device_eval_batch_size: 6
  gradient_accumulation_steps: 1
  learning_rate: 0.00025
  num_train_epochs: 1
  warmup_steps: 200
  weight_decay: 0.01
  logging_steps: 50
  evaluation_strategy: "steps"
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  fp16: true
  bf16: false
  gradient_checkpointing: true     # helps VRAM
  # W&B integration
  report_to="wandb",                 # << turn it on
  run_name="micro-<timestamp>",        # appears in W&B UI
