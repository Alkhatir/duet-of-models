output_dir: "./runs/tiny-gpt2-nano"
seed: 42

model:
  architecture: "gpt2"
  vocab_size: 1024          # <- set to your vocab
  hidden_size: 256
  num_hidden_layers: 6
  num_attention_heads: 8
  max_position_embeddings: 1024  # set to your training block_size


train:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 0.0003
  num_train_epochs: 1
  warmup_steps: 200
  weight_decay: 0.0
  logging_steps: 50
  evaluation_strategy: "steps"
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  fp16: true
  bf16: false
  gradient_checkpointing: True
  # W&B integration
  report_to="wandb",                 # << turn it on
  run_name="nano-<timestamp>",        # appears in W&B UI
