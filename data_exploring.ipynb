{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560902cd",
   "metadata": {},
   "source": [
    "# Loading Data to VM and setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, tqdm, json, os, math\n",
    "\n",
    "ROOT = Path(\"data/lmd_matched\").expanduser()\n",
    "midi_files = sorted(ROOT.rglob(\"*.mid\")) + sorted(ROOT.rglob(\"*.midi\"))\n",
    "print(\"Number of MIDIs:\", len(midi_files))\n",
    "print(\"Here are three examples:\", midi_files[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2f2a6",
   "metadata": {},
   "source": [
    "# Basic integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c9ccc",
   "metadata": {},
   "source": [
    "## A glimpse into the type of music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b08383",
   "metadata": {},
   "source": [
    "the following graph tells us how the models are going to behave according to the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_distro = pd.read_csv(\"data/term_distro.csv\", delimiter=\",\")\n",
    "mbtags_distro = pd.read_csv(\"data/mbtag_distro.csv\", delimiter=\",\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), tight_layout=True)\n",
    "\n",
    "ax1.bar(terms_distro[\"genre_term\"].loc[:20], terms_distro[\"n_tracks\"].loc[:20])\n",
    "ax1.set_title(\"Distribution of genre terms\", fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Number of tracks\")\n",
    "ax1.set_xlabel(\"Genre term\")\n",
    "ax1.tick_params(axis=\"x\", labelrotation=45)\n",
    "for t in ax1.get_xticklabels():\n",
    "    t.set_ha(\"right\")\n",
    "\n",
    "ax2.bar(mbtags_distro[\"genre_tag\"].loc[:20], mbtags_distro[\"n_tracks\"].loc[:20])\n",
    "ax2.set_title(\"Distribution of musicbrainz tags\", fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Number of tracks\")\n",
    "ax2.set_xlabel(\"Genre tag\")\n",
    "ax2.tick_params(axis=\"x\", labelrotation=45)\n",
    "for t in ax2.get_xticklabels():\n",
    "    t.set_ha(\"right\")\n",
    "plt.savefig(\"data_reports/genre_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e80e3b",
   "metadata": {},
   "source": [
    "## Some statistics on a sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi, random, statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "rng = random.Random()\n",
    "sample = rng.sample(midi_files, k=5000)\n",
    "\n",
    "\n",
    "def safe_read(p):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(str(p))\n",
    "        return pm\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "rows = []\n",
    "for p in tqdm(sample):\n",
    "    pm = safe_read(p)\n",
    "    ok = pm is not None\n",
    "    dur = pm.get_end_time() if ok else np.nan\n",
    "    tempos = [] if not ok else [t for t in pm.get_tempo_changes()[1]]\n",
    "    time_sigs = (\n",
    "        []\n",
    "        if not ok\n",
    "        else [(ts.numerator, ts.denominator) for ts in pm.time_signature_changes]\n",
    "    )\n",
    "    n_notes = 0 if not ok else sum(len(i.notes) for i in pm.instruments)\n",
    "    n_instr = 0 if not ok else len(set([i.program for i in pm.instruments]))\n",
    "    rows.append(\n",
    "        dict(\n",
    "            path=str(p),\n",
    "            ok=ok,\n",
    "            duration=dur,\n",
    "            notes=n_notes,\n",
    "            instruments=n_instr,\n",
    "            tempi=len(tempos),\n",
    "            time_sigs=len(time_sigs),\n",
    "        )\n",
    "    )\n",
    "df_basic = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85f517",
   "metadata": {},
   "source": [
    "### Creating the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_basic, title=\"Data Report\", explorative=True)\n",
    "profile.to_notebook_iframe() # open in your browser\n",
    "profile.to_file(\"data_reports/basic_info.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b128a5",
   "metadata": {},
   "source": [
    "# Claculating some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64315cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import muspy\n",
    "# --- setup tallies ---\n",
    "prog_ctr, drum_ctr = Counter(), 0\n",
    "all_pitches = []          \n",
    "rows = []\n",
    "\n",
    "for p in tqdm(sample):\n",
    "    pm = safe_read(p)\n",
    "    if not pm:\n",
    "        continue\n",
    "\n",
    "    # per-instrument bookkeeping (from PrettyMIDI side)\n",
    "    for inst in pm.instruments:\n",
    "        if inst.is_drum:\n",
    "            drum_ctr += 1\n",
    "        else:\n",
    "            prog_ctr[inst.program] += 1\n",
    "        # collect all pitches for global stats\n",
    "        all_pitches.extend(n.pitch for n in inst.notes)\n",
    "\n",
    "    # switch to MusPy for metrics\n",
    "    m = muspy.from_pretty_midi(pm)\n",
    "\n",
    "    # --- measure resolution (robust) ---\n",
    "    def safe_measure_resolution(m):\n",
    "        # first valid TS (denominator > 0)\n",
    "        ts = next((ts for ts in m.time_signatures if getattr(ts, \"denominator\", 0) > 0), None)\n",
    "        if ts is None:\n",
    "            num, den = 4, 4\n",
    "        else:\n",
    "            num, den = ts.numerator, ts.denominator\n",
    "            if den <= 0:\n",
    "                den = 4\n",
    "        # resolution (ticks/qn) * (num beats per measure) * (4/den) to normalize to quarter=1\n",
    "        return int(round(m.resolution * num * 4 / den))\n",
    "\n",
    "    measure_resolution = safe_measure_resolution(m)\n",
    "\n",
    "    # --- counts & duration ---\n",
    "    total_notes = sum(len(t.notes) for t in m.tracks)\n",
    "    non_drum_notes = [n for t in m.tracks if not t.is_drum for n in t.notes]\n",
    "    drum_notes = [n for t in m.tracks if t.is_drum for n in t.notes]\n",
    "\n",
    "    # whole-piece duration in ticks (MusPy)\n",
    "    dur_ticks = m.get_end_time() or 0\n",
    "    beat = m.resolution\n",
    "    dur_seconds = pm.get_end_time() or 0.0  \n",
    "\n",
    "    # --- validity & hygiene ---\n",
    "    def overlaps_same_pitch(track):\n",
    "        bad = 0\n",
    "        # already ordered per pitch by time thanks to key sort:\n",
    "        notes = sorted(track.notes, key=lambda x: (x.pitch, x.time, x.end))\n",
    "        by_pitch = {}\n",
    "        for n in notes:\n",
    "            by_pitch.setdefault(n.pitch, []).append((n.time, n.end))\n",
    "        for spans in by_pitch.values():\n",
    "            last_end = -1\n",
    "            for t0, t1 in spans:\n",
    "                if t0 < last_end:\n",
    "                    bad += 1\n",
    "                if t1 > last_end:\n",
    "                    last_end = t1\n",
    "        return bad\n",
    "\n",
    "    same_pitch_overlap_count = sum(overlaps_same_pitch(t) for t in m.tracks if not t.is_drum)\n",
    "    ts_changes = max(len(m.time_signatures) - 1, 0)\n",
    "    parsable_nonempty = int(total_notes > 0 and dur_seconds > 0)\n",
    "\n",
    "    # --- tempo stats ---\n",
    "    # optional: dedup consecutive tempo qpm values at the same time\n",
    "    tempos = m.tempos\n",
    "    if tempos:\n",
    "        # (time, qpm) sorted; drop exact duplicates by (time,qpm)\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for tm in sorted(tempos, key=lambda x: getattr(x, \"time\", 0)):\n",
    "            key = (getattr(tm, \"time\", 0), tm.qpm)\n",
    "            if key not in seen:\n",
    "                uniq.append(tm)\n",
    "                seen.add(key)\n",
    "        bpms = [tm.qpm for tm in uniq]\n",
    "    else:\n",
    "        bpms = []\n",
    "\n",
    "    tempo_mean_bpm = (sum(bpms) / len(bpms)) if bpms else float(\"nan\")\n",
    "    tempo_std_bpm = ((sum((x - tempo_mean_bpm) ** 2 for x in bpms) / len(bpms)) ** 0.5) if len(bpms) > 1 else 0.0\n",
    "    tempo_changes = max(len(bpms) - 1, 0)\n",
    "\n",
    "    # --- grid alignment (16ths) ---\n",
    "    grid16 = max(int(round(beat / 4)), 1)\n",
    "    off = sum((n.time % grid16) != 0 for t in m.tracks for n in t.notes)\n",
    "    offgrid_rate_16 = off / max(total_notes, 1)\n",
    "\n",
    "    # --- measure counts & density variance ---\n",
    "    length_ticks = m.get_end_time() or 0\n",
    "    n_measures = int(length_ticks // measure_resolution) + (1 if length_ticks > 0 else 0)\n",
    "    counts_per_measure = [\n",
    "        sum(1 for t in m.tracks for n in t.notes if (n.time // measure_resolution) == k)\n",
    "        for k in range(n_measures)\n",
    "    ]\n",
    "    if counts_per_measure:\n",
    "        mean_c = sum(counts_per_measure) / len(counts_per_measure)\n",
    "        onset_density_var_meas = sum((c - mean_c) ** 2 for c in counts_per_measure) / len(counts_per_measure)\n",
    "    else:\n",
    "        onset_density_var_meas = float(\"nan\")\n",
    "\n",
    "    # --- dynamics (non-drum) ---\n",
    "    vels = [n.velocity for n in non_drum_notes]\n",
    "    if vels:\n",
    "        sv = sorted(vels)\n",
    "        def pct(sv, p):  # simple nearest-rank percentile\n",
    "            k = max(0, min(len(sv)-1, int(round(p * (len(sv)-1)))))\n",
    "            return sv[k]\n",
    "        v_mean = sum(sv) / len(sv)\n",
    "        velocity_mean = v_mean\n",
    "        velocity_std = (sum((v - v_mean) ** 2 for v in sv) / len(sv)) ** 0.5\n",
    "        velocity_p95_minus_p5 = pct(sv, 0.95) - pct(sv, 0.05)\n",
    "    else:\n",
    "        velocity_mean = velocity_std = velocity_p95_minus_p5 = float(\"nan\")\n",
    "\n",
    "    # --- articulation ---\n",
    "    # average note duration in beats:\n",
    "    articulation_beats = (sum(n.duration for t in m.tracks for n in t.notes) / max(total_notes * beat, 1)) if total_notes else float(\"nan\")\n",
    "\n",
    "    # --- instrumentation & meta ---\n",
    "    track_count = len(m.tracks)\n",
    "    drum_note_ratio = len(drum_notes) / max(total_notes, 1)\n",
    "    pitch_register_mean = (sum(n.pitch for n in non_drum_notes) / len(non_drum_notes)) if non_drum_notes else float(\"nan\")\n",
    "    length_measures = n_measures\n",
    "\n",
    "    rows.append(dict(\n",
    "        path=str(p),\n",
    "        polyphony=muspy.metrics.polyphony(m),\n",
    "        polyphony_rate=muspy.metrics.polyphony_rate(m),\n",
    "        # per-second density (uses PrettyMIDI seconds):\n",
    "        notes_density=total_notes / max(dur_seconds, 1e-6),\n",
    "        empty_beat_rate=muspy.metrics.empty_beat_rate(m),\n",
    "        empty_measure_rate=muspy.metrics.empty_measure_rate(m, measure_resolution),\n",
    "        groove_consistency=muspy.metrics.groove_consistency(m, measure_resolution),\n",
    "        n_pitch_classes_used=muspy.metrics.n_pitch_classes_used(m),\n",
    "        n_pitches_used=muspy.metrics.n_pitches_used(m),\n",
    "        pitch_class_entropy=muspy.metrics.pitch_class_entropy(m),\n",
    "        pitch_entropy=muspy.metrics.pitch_entropy(m),\n",
    "        pitch_range=muspy.metrics.pitch_range(m),\n",
    "        scale_consistency=muspy.metrics.scale_consistency(m),\n",
    "        same_pitch_overlap_count=same_pitch_overlap_count,\n",
    "        time_signature_changes=ts_changes,\n",
    "        parsable_nonempty=parsable_nonempty,\n",
    "        tempo_mean_bpm=tempo_mean_bpm,\n",
    "        tempo_std_bpm=tempo_std_bpm,\n",
    "        tempo_changes=tempo_changes,\n",
    "        offgrid_rate_16=offgrid_rate_16,\n",
    "        onset_density_var_meas=onset_density_var_meas,\n",
    "        velocity_mean=velocity_mean,\n",
    "        velocity_std=velocity_std,\n",
    "        velocity_p95_minus_p5=velocity_p95_minus_p5,\n",
    "        articulation_avg_beats=articulation_beats,  # renamed for clarity\n",
    "        drum_note_ratio=drum_note_ratio,\n",
    "        pitch_register_mean=pitch_register_mean,\n",
    "        length_measures=length_measures,\n",
    "        track_count=track_count,\n",
    "    ))\n",
    "# build df\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "# global pitch stats over all notes:\n",
    "pd.Series(all_pitches).describe(), prog_ctr.most_common(10), drum_ctr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84405aa4",
   "metadata": {},
   "source": [
    "## Generating the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_metrics, title=\"Metrics Report\", explorative=True)\n",
    "profile.to_notebook_iframe() # open in your browser\n",
    "profile.to_file(\"data_reports/metrics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c946e8e",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fb004",
   "metadata": {},
   "source": [
    "### 🔥 Strongest Correlations\n",
    "\n",
    "#### Near-duplicates\n",
    "- **Tempo-related**\n",
    "  - `tempo_changes ↔ tempo_std_bpm` **0.954**  \n",
    "    More tempo changes → higher tempo variability.\n",
    "- **Dynamics-related**\n",
    "  - `velocity_p95_minus_p5 ↔ velocity_std` **0.951**  \n",
    "    Both measure spread of dynamics.\n",
    "- **Sparsity-related**\n",
    "  - `empty_beat_rate ↔ empty_measure_rate` **0.863**  \n",
    "    Sparse beats go with sparse measures.\n",
    "- **Pitch/tonal clarity**\n",
    "  - `pitch_class_entropy ↔ scale_consistency` **−0.848**  \n",
    "    More tonal ambiguity → less scale consistency.\n",
    "- **Pitch diversity**\n",
    "  - `n_pitches_used ↔ pitch_entropy` **0.798**  \n",
    "    More unique pitches → higher pitch entropy.\n",
    "- **Texture**\n",
    "  - `polyphony ↔ polyphony_rate` **0.790**  \n",
    "    Essentially the same concept.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎵 Pitch & Harmony Cluster\n",
    "- `n_pitch_classes_used ↔ n_pitches_used` **0.764**\n",
    "- `pitch_class_entropy ↔ pitch_entropy` **0.759**\n",
    "- `n_pitch_classes_used ↔ pitch_class_entropy` **0.693**\n",
    "- `n_pitches_used ↔ pitch_class_entropy` **0.692**\n",
    "- `n_pitches_used ↔ scale_consistency` **−0.650**\n",
    "- `n_pitches_used ↔ pitch_range` **0.638**\n",
    "- `pitch_entropy ↔ scale_consistency` **−0.635**\n",
    "\n",
    "**Interpretation:**  \n",
    "Greater pitch variety leads to higher entropy, wider pitch range, and weaker adherence to a single scale.\n",
    "\n",
    "---\n",
    "\n",
    "### 🥁 Rhythm & Tightness\n",
    "- `groove_consistency ↔ offgrid_rate_16` **−0.721**  \n",
    "  More off-grid 16ths → lower groove consistency.\n",
    "- `notes_density ↔ onset_density_var_meas` **0.622**  \n",
    "  Denser textures → greater onset variance.\n",
    "- `articulation_ratio ↔ notes_density` **−0.527**  \n",
    "  Dense notes → shorter articulations.\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ Tempo & Length\n",
    "- `length_measures ↔ tempo_mean_bpm` **0.536**  \n",
    "  Longer pieces → higher mean BPM (dataset-specific effect).\n",
    "- `notes_density ↔ tempo_mean_bpm` **0.442**  \n",
    "  Faster tempos → denser textures.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎼 Polyphony & Articulation\n",
    "- `articulation_ratio ↔ polyphony_rate` **0.506**\n",
    "- `articulation_ratio ↔ polyphony` **0.425**  \n",
    "\n",
    "**Interpretation:**  \n",
    "Higher note overlap (polyphony) is associated with longer articulations.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Practical Takeaways\n",
    "\n",
    "#### Feature Selection\n",
    "To reduce **multicollinearity**, drop one from each of these pairs:\n",
    "- `tempo_changes` vs. `tempo_std_bpm`\n",
    "- `velocity_p95_minus_p5` vs. `velocity_std`\n",
    "- `empty_beat_rate` vs. `empty_measure_rate`\n",
    "- `polyphony` vs. `polyphony_rate`\n",
    "- Among pitch/tonal measures, keep a compact subset (e.g., `n_pitch_classes_used`, `scale_consistency`, `pitch_entropy`).\n",
    "\n",
    "#### Latent Factors (useful for PCA or clustering)\n",
    "- **Tonal variety / ambiguity:** (`pitch_entropy`, `pitch_class_entropy`, `n_pitches_used`, inverse of `scale_consistency`)\n",
    "- **Rhythmic tightness vs looseness:** (`groove_consistency` vs. `offgrid_rate_16`)\n",
    "- **Density / energy:** (`notes_density`, `onset_density_var_meas`, `tempo_mean_bpm`)\n",
    "- **Dynamics spread:** (`velocity_std` or `velocity_p95_minus_p5`)\n",
    "- **Texture:** (`polyphony`, `polyphony_rate`, linked to `articulation_ratio`)\n",
    "\n",
    "#### Sanity Check\n",
    "- Off-grid timing decreases groove consistency (expected).\n",
    "- More pitch classes reduce tonal stability (expected).\n",
    "- Denser passages encourage staccato articulation (musically intuitive).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a90ae",
   "metadata": {},
   "source": [
    "# Pianoroll Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pianoroll(pm, t0=0, t1=30):\n",
    "    import numpy as np\n",
    "\n",
    "    # coarse roll\n",
    "    sr = 20  # frames per second (simple visual)\n",
    "    T = int((t1 - t0) * sr)\n",
    "    roll = np.zeros((128, T))\n",
    "    for inst in pm.instruments:\n",
    "        for n in inst.notes:\n",
    "            s = int(max(0, (n.start - t0) * sr))\n",
    "            e = int(min(T, (n.end - t0) * sr))\n",
    "            if e > s:\n",
    "                roll[n.pitch, s:e] = 1\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(roll[::-1], aspect=\"auto\", interpolation=\"nearest\")\n",
    "    plt.xlabel(\"time frames\")\n",
    "    plt.ylabel(\"MIDI pitch (high at top)\")\n",
    "    plt.title(\"Pianoroll ({}–{}s)\".format(t0, t1))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pm = safe_read(sample[0])\n",
    "show_pianoroll(pm, 0, int(min(30, pm.get_end_time())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
